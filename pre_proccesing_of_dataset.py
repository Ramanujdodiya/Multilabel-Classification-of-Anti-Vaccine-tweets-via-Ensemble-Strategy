# -*- coding: utf-8 -*-
"""Pre-Proccesing of dataset

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TKrOkvWQEZG-AsvErZsiXBDYxr3MvN2t
"""

from google.colab import drive
drive.mount('/content/drive')
import json
# loading the data from Gdrive
with open('/content/drive/My Drive/Colab Notebooks/CAVES_json/combined.json', "r") as file:
    dataset = json.load(file)

# importing all the dependencies
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# taking a look at top 5 rows
for entry in dataset[:5]:
    print(entry)
    print("\n---\n")

df = pd.DataFrame(dataset)
# Display the distribution of labels
labels_counts = df['labels'].apply(lambda x: list(x.keys())).explode().value_counts()
def extract_keys_from_dict(dictionary):
    if isinstance(dictionary, dict):
        return ', '.join(dictionary.keys())
    else:
        return None

def clean_and_remove_stopwords(text):
    cleaned_text = re.sub(r'[^a-zA-Z\s]', '', str(text)).lower()
    words = cleaned_text.split()
    words = [word for word in words if word not in stop_words]
    return ' '.join(words)

def PreProcess(dataset):
    dataset['keys'] = dataset['labels'].apply(extract_keys_from_dict)
    dataset_2=dataset.drop(columns=['labels'],axis=0)
    # Spliting the comma-separated labels and convert them to lists
    dataset_2['keys'] = dataset_2['keys'].apply(lambda x: x.split(', '))
    # Using explode to create separate rows for each label
    dataset_3 = dataset_2.explode('keys')
    # tweets
    dataset_3['tweet'] = dataset_3['tweet'].apply(clean_and_remove_stopwords)
    return dataset_3


df=PreProcess(dataset)
df.drop(columns=["ID"])
df['count'] = df['tweet'].apply(lambda x: len(x.split()))
df=df.rename(columns={'newlabel':'label'})
df=df.rename(columns={'tweet':'text'})
df.head()
category_count = df['keys'].value_counts()
categories = category_count.index
print(categories)
print(category_count)   
df['label'] = df['label'].apply(lambda x: ast.literal_eval(x))
df.to_csv('/content/drive/My Drive/Colab Notebooks/CAVES_json/dataset_updated.csv',index=False)